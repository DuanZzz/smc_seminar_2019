
@article{spagnol_viking_2019,
	title = {{THE} {VIKING} {HRTF} {DATASET}},
	abstract = {This paper describes the Viking HRTF dataset, a collection of head-related transfer functions (HRTFs) measured at the University of Iceland. The dataset includes fullsphere HRTFs measured on a dense spatial grid (1513 positions) with a KEMAR mannequin with 20 different artiﬁcial left pinnae attached, one at a time. The artiﬁcial pinnae were previously obtained through a custom molding procedure from 20 different lifelike human heads. The analyses of results reported here suggest that the collected acoustical measurements are robust, reproducible, and faithful to reference KEMAR HRTFs, and that material hardness has a negligible impact on the measurements compared to pinna shape. The purpose of the present collection, which is available for free download, is to provide accurate input data for future investigations on the relation between HRTFs and anthropometric data through machine learning techniques or other state-of-the-art methodologies.},
	language = {en},
	author = {Spagnol, Simone and Purkhús, Kristján Bjarki and Unnthórsson, Rúnar and Björnsson, Sverrir Karl},
	year = {2019},
	pages = {6},
	file = {Spagnol et al. - 2019 - THE VIKING HRTF DATASET.pdf:/Users/miccio/Zotero/storage/ESP26SVA/Spagnol et al. - 2019 - THE VIKING HRTF DATASET.pdf:application/pdf}
}

@article{lee_personalized_2018,
	title = {Personalized {HRTF} {Modeling} {Based} on {Deep} {Neural} {Network} {Using} {Anthropometric} {Measurements} and {Images} of the {Ear}},
	volume = {8},
	issn = {2076-3417},
	url = {http://www.mdpi.com/2076-3417/8/11/2180},
	doi = {10.3390/app8112180},
	abstract = {This paper proposes a personalized head-related transfer function (HRTF) estimation method based on deep neural networks by using anthropometric measurements and ear images. The proposed method consists of three sub-networks for representing personalized features and estimating the HRTF. As input features for neural networks, the anthropometric measurements regarding the head and torso are used for a feedforward deep neural network (DNN), and the ear images are used for a convolutional neural network (CNN). After that, the outputs of these two sub-networks are merged into another DNN for estimation of the personalized HRTF. To evaluate the performance of the proposed method, objective and subjective evaluations are conducted. For the objective evaluation, the root mean square error (RMSE) and the log spectral distance (LSD) between the reference HRTF and the estimated one are measured. Consequently, the proposed method provides the RMSE of −18.40 dB and LSD of 4.47 dB, which are lower by 0.02 dB and higher by 0.85 dB than the DNN-based method using anthropometric data without pinna measurements, respectively. Next, a sound localization test is performed for the subjective evaluation. As a result, it is shown that the proposed method can localize sound sources with higher accuracy of around 11\% and 6\% than the average HRTF method and DNN-based method, respectively. In addition, the reductions of the front/back confusion rate by 12.5\% and 2.5\% are achieved by the proposed method, compared to the average HRTF method and DNN-based method, respectively.},
	language = {en},
	number = {11},
	urldate = {2019-08-23},
	journal = {Applied Sciences},
	author = {Lee, Geon and Kim, Hong},
	month = nov,
	year = {2018},
	pages = {2180},
	file = {Lee and Kim - 2018 - Personalized HRTF Modeling Based on Deep Neural Ne.pdf:/Users/miccio/Zotero/storage/3KZGB8C6/Lee and Kim - 2018 - Personalized HRTF Modeling Based on Deep Neural Ne.pdf:application/pdf}
}

@incollection{hutchison_adaptive_2012,
	address = {Berlin, Heidelberg},
	title = {Adaptive {Modeling} of {HRTFs} {Based} on {Reinforcement} {Learning}},
	volume = {7666},
	isbn = {978-3-642-34477-0 978-3-642-34478-7},
	url = {http://link.springer.com/10.1007/978-3-642-34478-7_52},
	abstract = {Although recent studies on out-of-head sound localization technology have been aimed at applications in entertainment, this technology can also be used to provide an interface to connect a computer to the human brain. An eﬀective out-of-head system requires an accurate head-related transfer function (HRTF). However, it is diﬃcult to measure HRTF accurately. We propose a new method based on reinforcement learning to estimate HRTF accurately from measurement data and validate it through simulations. We used the actor-critic paradigm to learn the HRTF parameters and the autoregressive moving average (ARMA) model to reduce the number of such parameters. Our simulations suggest that an accurate HRTF can be estimated with this method. The proposed method is expected to be useful for not only entertainment applications but also brain-machine-interface (BMI) based on out-of-head sound localization technology.},
	language = {en},
	urldate = {2019-08-23},
	booktitle = {Neural {Information} {Processing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Morioka, Shuhei and Nambu, Isao and Yano, Shohei and Hokari, Haruhide and Wada, Yasuhiro},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Huang, Tingwen and Zeng, Zhigang and Li, Chuandong and Leung, Chi Sing},
	year = {2012},
	doi = {10.1007/978-3-642-34478-7_52},
	pages = {423--430},
	file = {Morioka et al. - 2012 - Adaptive Modeling of HRTFs Based on Reinforcement .pdf:/Users/miccio/Zotero/storage/UEH9CQCR/Morioka et al. - 2012 - Adaptive Modeling of HRTFs Based on Reinforcement .pdf:application/pdf;Morioka et al. - 2012 - Adaptive Modeling of HRTFs Based on Reinforcement .pdf:/Users/miccio/Zotero/storage/NASSKBVE/Morioka et al. - 2012 - Adaptive Modeling of HRTFs Based on Reinforcement .pdf:application/pdf}
}

@inproceedings{kestler_head_2019,
	address = {Brighton, United Kingdom},
	title = {Head {Related} {Impulse} {Response} {Interpolation} and {Extrapolation} {Using} {Deep} {Belief} {Networks}},
	isbn = {978-1-4799-8131-1},
	url = {https://ieeexplore.ieee.org/document/8683570/},
	doi = {10.1109/ICASSP.2019.8683570},
	abstract = {This paper presents a machine learning Deep Belief Network technique for interpolation and extrapolation of HRTF (Head Related Transfer Function) databases. In this technique, we decouple the stereo pair of HRTFs for each ear. Furthermore, we remove the inter-aural time differences (ITD) and distance attenuation from the recorded HRTF measurements such that the DBNs only interpolate and extrapolate the spectrum ﬁltering of the audio for the two ears. Testing on the CIPIC and SCUT databases produces results of an average log spectral distortion less than 3 dB for all points around the head.},
	language = {en},
	urldate = {2019-08-23},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Kestler, Grady and Yadegari, Shahrokh and Nahamoo, David},
	month = may,
	year = {2019},
	pages = {266--270},
	file = {Kestler et al. - 2019 - Head Related Impulse Response Interpolation and Ex.pdf:/Users/miccio/Zotero/storage/5HVUHZT2/Kestler et al. - 2019 - Head Related Impulse Response Interpolation and Ex.pdf:application/pdf}
}

@article{hu_hrtf_2008,
	title = {{HRTF} personalization based on artificial neural network in individual virtual auditory space},
	volume = {69},
	issn = {0003682X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0003682X07000965},
	doi = {10.1016/j.apacoust.2007.05.007},
	language = {en},
	number = {2},
	urldate = {2019-08-23},
	journal = {Applied Acoustics},
	author = {Hu, Hongmei and Zhou, Lin and Ma, Hao and Wu, Zhenyang},
	month = feb,
	year = {2008},
	pages = {163--172},
	file = {Hu et al. - 2008 - HRTF personalization based on artificial neural ne.pdf:/Users/miccio/Zotero/storage/8UTCMQJ7/Hu et al. - 2008 - HRTF personalization based on artificial neural ne.pdf:application/pdf}
}

@misc{hens_zimmerman_deep_2018,
	title = {Deep {Learning} as a predictor for personalized head related transfer functions in virtual environments},
	url = {https://www.researchgate.net/publication/327582823_Deep_Learning_as_a_predictor_for_personalized_head_related_transfer_functions_in_virtual_environments/related},
	abstract = {One of the most impressive capabilities of the human auditory system is the ability to localize the origin of a sound source in free-field listening. This capacity is partially lost when wearing headphones. Individualizing spatial audio is therefore of crucial importance to authentic localization in virtual and augmented reality environments. In this paper, a method for individualizing spatial audio by relating pinna anatomy data to known pinnae and matching Head Related Transfer Functions (HRTFs) using Deep Learning is presented. The HRTF is a function that characterizes the response of a given individual to sound from a particular location in an egocentric coordinate system. The HRTF displays considerable inter-personal variability, and an open problem is the development of a generative model for the HRTF from personal anthropometric data. In this paper, a pre-trained Convolutional Neural Network (CNN) is used to estimate an individualized HRTF from a photograph of the participant’s left pinna. The hypothesis states that auditory localization in a binaural listening environment can be improved by pre-selecting a Head Related Impulse Response (HRIR) from a known database of filters, based on a photograph of a subject’s pinna. The aim of the experiment was to compare the apparent positions of sounds, presented over headphones, with software defined locations, where treatments were presented with an individual HRTF and controls were presented with a generic HRTF. Both treatment and control data was collected from a group of random participants (N = 34, age 7 – 76) in a repeated measures design. The proposed estimator delivers smaller but not significant (α= 0.05) discrepancy from the sound source location with a smaller dispersion in the axial plane, compared to the case of using an average control, when minors are excluded from the test group (N = 24, age 18 - 76). The method could provide a convenient way of HRTF individualization without the need of tedious measurements or the use of expensive 3D scanners. Preliminary results of objective evaluations suggest that the proposed method will enjoy further improvements with obvious benefits for the creative media industry, specifically those segments concerned with consumer or professional grade virtual and augmented reality applications.},
	language = {en},
	urldate = {2019-08-23},
	journal = {ResearchGate},
	author = {{Hens Zimmerman}},
	year = {2018},
	file = {Hens Zimmerman - 2018 - Deep Learning as a predictor for personalized head.pdf:/Users/miccio/Zotero/storage/IHSIFZJN/Hens Zimmerman - 2018 - Deep Learning as a predictor for personalized head.pdf:application/pdf;Snapshot:/Users/miccio/Zotero/storage/7T7QXZ93/related.html:text/html}
}

@article{yamamoto_fully_2017,
	title = {Fully perceptual-based 3D spatial sound individualization with an adaptive variational autoencoder},
	volume = {36},
	issn = {07300301},
	url = {http://dl.acm.org/citation.cfm?doid=3130800.3130838},
	doi = {10.1145/3130800.3130838},
	language = {en},
	number = {6},
	urldate = {2019-08-23},
	journal = {ACM Transactions on Graphics},
	author = {Yamamoto, Kazuhiko and Igarashi, Takeo},
	month = nov,
	year = {2017},
	pages = {1--13},
	file = {Yamamoto and Igarashi - 2017 - Fully perceptual-based 3D spatial sound individual.pdf:/Users/miccio/Zotero/storage/VC3VV4DS/Yamamoto and Igarashi - 2017 - Fully perceptual-based 3D spatial sound individual.pdf:application/pdf}
}

@inproceedings{luo_virtual_2013,
	address = {New Paltz, NY, USA},
	title = {Virtual autoencoder based recommendation system for individualizing head-related transfer functions},
	isbn = {978-1-4799-0972-8},
	url = {http://ieeexplore.ieee.org/document/6701816/},
	doi = {10.1109/WASPAA.2013.6701816},
	abstract = {We propose a virtual autoencoder based recommendation system for learning a user’s Head-related Transfer Functions (HRTFs) without subjecting a listener to impulse response or anthropometric measurements. Autoencoder neural-networks generalize principal component analysis (PCA) and learn non-linear feature spaces that supports both out-of-sample embedding and reconstruction; this may be applied to developing a more expressive low-dimensional HRTF representation. One application is to individualize HRTFs by tuning along the autoencoder feature spaces. We demonstrate this new approach by developing a virtual (black-box) user that can localize sound from query HRTFs reconstructed from those spaces. Standard optimization methods tune the autoencoder features based on the virtual user’s feedback. Experiments with CIPIC HRTFs show that the virtual user can localize along out-of-sample directions and that optimization in the autoencoder feature space improves upon initial non-individualized HRTFs. Other applications of the representation are also discussed.},
	language = {en},
	urldate = {2019-08-23},
	booktitle = {2013 {IEEE} {Workshop} on {Applications} of {Signal} {Processing} to {Audio} and {Acoustics}},
	publisher = {IEEE},
	author = {Luo, Yuancheng and Zotkin, Dmitry N. and Duraiswami, Ramani},
	month = oct,
	year = {2013},
	keywords = {Acoustics, anthropometric measurements, Autoencoder, autoencoder features, autoencoder neural-networks, black box, CIPIC HRTF, codecs, Conferences, Gaussian Process Regression, Head-related Transfer Function, head-related transfer functions, impulse response, low-dimensional HRTF representation, neural nets, nonlinear feature spaces, optimisation, Optimization, out-of-sample embedding, PCA, principal component analysis, Principal component analysis, Signal processing, standard optimization methods, telecommunication computing, Training, transient response, Vectors, virtual autoencoder based recommendation system, virtual user feedback},
	pages = {1--4},
	file = {IEEE Xplore Abstract Record:/Users/miccio/Zotero/storage/YQDJUMEA/6701816.html:text/html;Luo et al. - 2013 - Virtual autoencoder based recommendation system fo.pdf:/Users/miccio/Zotero/storage/KK5ZTHS3/Luo et al. - 2013 - Virtual autoencoder based recommendation system fo.pdf:application/pdf;Luo et al. - 2013 - Virtual autoencoder based recommendation system fo.pdf:/Users/miccio/Zotero/storage/HCVG2HV4/Luo et al. - 2013 - Virtual autoencoder based recommendation system fo.pdf:application/pdf}
}

@article{snaidero_measuring_2011,
	title = {Measuring {HRTFs} of {Brüel} \&amp; {Kjær} {Type} 4128-{C}, {G}.{R}.{A}.{S}. {KEMAR} {Type} 45BM, and {Head} {Acoustics} {HMS} {II}.3 {Head} and {Torso} {Simulators}},
	author = {Snaidero, Thomas},
	year = {2011},
	file = {Snaidero - 2011 - Measuring HRTFs of Brüel &amp\; Kjær Type 4128-C, G.html:/Users/miccio/Zotero/storage/2PQRT5P5/Snaidero - 2011 - Measuring HRTFs of Brüel &amp\; Kjær Type 4128-C, G.html:text/html;Snaidero - 2011 - Measuring HRTFs of Brüel &amp\; Kjær Type 4128-C, G.pdf:/Users/miccio/Zotero/storage/NXJZZYN7/Snaidero - 2011 - Measuring HRTFs of Brüel &amp\; Kjær Type 4128-C, G.pdf:application/pdf}
}

@misc{noauthor_spatial_nodate,
	title = {Spatial {Audio}},
	url = {https://www.microsoft.com/en-us/research/project/spatial-audio/},
	abstract = {Spatial audio, also known as 3D stereo sound, is about creating a 3D audio experience by using headphones. Applications of this technology include augmented and virtual reality, listening to music, and watching a movie on a tablet or PC.   Head related transfer functions Head-related transfer functions (HRTFs) are measurements that describe the directivity patterns …},
	language = {en-US},
	urldate = {2019-08-23},
	journal = {Microsoft Research},
	file = {Snapshot:/Users/miccio/Zotero/storage/JLSUN9PK/spatial-audio.html:text/html}
}

@article{luo_gaussian_2015,
	title = {Gaussian {Process} {Models} for {HRTF} based {Sound}-{Source} {Localization} and {Active}-{Learning}},
	url = {http://arxiv.org/abs/1502.03163},
	abstract = {From a machine learning perspective, the human ability localize sounds can be modeled as a non-parametric and non-linear regression problem between binaural spectral features of sound received at the ears (input) and their sound-source directions (output). The input features can be summarized in terms of the individual's head-related transfer functions (HRTFs) which measure the spectral response between the listener's eardrum and an external point in \$3\$D. Based on these viewpoints, two related problems are considered: how can one achieve an optimal sampling of measurements for training sound-source localization (SSL) models, and how can SSL models be used to infer the subject's HRTFs in listening tests. First, we develop a class of binaural SSL models based on Gaussian process regression and solve a {\textbackslash}emph\{forward selection\} problem that finds a subset of input-output samples that best generalize to all SSL directions. Second, we use an {\textbackslash}emph\{active-learning\} approach that updates an online SSL model for inferring the subject's SSL errors via headphones and a graphical user interface. Experiments show that only a small fraction of HRTFs are required for \$5{\textasciicircum}\{{\textbackslash}circ\}\$ localization accuracy and that the learned HRTFs are localized closer to their intended directions than non-individualized HRTFs.},
	urldate = {2019-08-23},
	journal = {arXiv:1502.03163 [cs, stat]},
	author = {Luo, Yuancheng and Zotkin, Dmitry N. and Duraiswami, Ramani},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.03163},
	keywords = {Computer Science - Sound, Computer Science - Machine Learning, Statistics - Machine Learning, 60G15},
	annote = {Comment: 11 pages},
	file = {arXiv.org Snapshot:/Users/miccio/Zotero/storage/K9YTLAHA/1502.html:text/html;Luo et al. - 2015 - Gaussian Process Models for HRTF based Sound-Sourc.pdf:/Users/miccio/Zotero/storage/DGGPHMJR/Luo et al. - 2015 - Gaussian Process Models for HRTF based Sound-Sourc.pdf:application/pdf}
}

@article{deleforge_acoustic_2013,
	title = {Acoustic {Space} {Mapping}: {A} {Machine} {Learning} {Approach} to {Sound} {Source} {Separation} and {Localization}},
	abstract = {In this thesis, we address the long-studied problem of binaural (two microphones) sound source separation and localization through supervised learning. To achieve this, we develop a new paradigm referred to as acoustic space mapping, at the crossroads of binaural perception, robot hearing, audio signal processing and machine learning. The proposed approach consists in learning a link between auditory cues perceived by the system and the emitting sound source position in another modality of the system, such as the visual space or the motor space. We propose new experimental protocols to automatically gather large training sets that associate such data. Obtained datasets are then used to reveal some fundamental intrinsic properties of acoustic spaces and lead to the development of a general family of probabilistic models for locally-linear high- to low-dimensional space mapping. We show that these models unify several existing regression and dimensionality reduction techniques, while encompassing a large number of new models that generalize previous ones. The properties and inference of these models are thoroughly detailed, and the prominent advantage of proposed methods with respect to state-of-the-art techniques is established on different space mapping applications, beyond the scope of auditory scene analysis. We then show how the proposed methods can be probabilistically extended to tackle the long-known cocktail party problem, i.e., accurately localizing one or several sound sources emitting at the same time in a real-word environment, and separate the mixed signals. We show that resulting techniques perform these tasks with an unequaled accuracy. This demonstrates the important role of learning and puts forwards the acoustic space mapping paradigm as a promising tool for robustly addressing the most challenging problems in computational binaural audition.},
	language = {en},
	author = {Deleforge, Antoine},
	year = {2013},
	pages = {134},
	file = {Deleforge - 2013 - Acoustic Space Mapping A Machine Learning Approac.pdf:/Users/miccio/Zotero/storage/5WHPVUMK/Deleforge - 2013 - Acoustic Space Mapping A Machine Learning Approac.pdf:application/pdf}
}

@article{fink_individualization_2015,
	title = {Individualization of head related transfer functions using principal component analysis},
	volume = {87},
	issn = {0003682X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0003682X14001753},
	doi = {10.1016/j.apacoust.2014.07.005},
	language = {en},
	urldate = {2019-08-23},
	journal = {Applied Acoustics},
	author = {Fink, Kimberly J. and Ray, Laura},
	month = jan,
	year = {2015},
	pages = {162--173}
}

@article{zeng_hybrid_2010,
	title = {A hybrid algorithm for selecting head-related transfer function based on similarity of anthropometric structures},
	volume = {329},
	issn = {0022460X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022460X1000221X},
	doi = {10.1016/j.jsv.2010.03.031},
	language = {en},
	number = {19},
	urldate = {2019-08-23},
	journal = {Journal of Sound and Vibration},
	author = {Zeng, Xiang-Yang and Wang, Shu-Guang and Gao, Li-Ping},
	month = sep,
	year = {2010},
	pages = {4093--4106},
	file = {Zeng et al. - 2010 - A hybrid algorithm for selecting head-related tran.pdf:/Users/miccio/Zotero/storage/NIW6X42L/Zeng et al. - 2010 - A hybrid algorithm for selecting head-related tran.pdf:application/pdf}
}

@article{spagnol_frequency_2015,
	title = {Frequency estimation of the first pinna notch in head-related transfer functions with a linear anthropometric model},
	abstract = {The relation between anthropometric parameters and Head-Related Transfer Function (HRTF) features, especially those due to the pinna, are not fully understood yet. In this paper we apply signal processing techniques to extract the frequencies of the main pinna notches (known as N1, N2, and N3) in the frontal part of the median plane and build a model relating them to 13 different anthropometric parameters of the pinna, some of which depend on the elevation angle of the sound source. Results show that while the considered anthropometric parameters are not able to approximate with sufﬁcient accuracy neither the N2 nor the N3 frequency, eight of them are sufﬁcient for modeling the frequency of N1 within a psychoacoustically acceptable margin of error. In particular, distances between the ear canal and the outer helix border are the most important parameters for predicting N1.},
	language = {en},
	author = {Spagnol, Simone and Avanzini, Federico},
	year = {2015},
	pages = {6},
	file = {Spagnol and Avanzini - 2015 - Faculty of Ind. Eng., Mech. Eng. and Computer Scie.pdf:/Users/miccio/Zotero/storage/P4XPDLP9/Spagnol and Avanzini - 2015 - Faculty of Ind. Eng., Mech. Eng. and Computer Scie.pdf:application/pdf}
}

@article{cheng_introduction_2001,
	title = {Introduction to {Head}-{Related} {Transfer} {Functions} ({HRTFs}): {Representations} of {HRTFs} in {Time}, {Frequency}, and {Space}},
	volume = {49},
	language = {en},
	number = {4},
	journal = {J Audio Eng Soc},
	author = {Cheng, Corey I},
	year = {2001},
	pages = {19},
	file = {Cheng - 2001 - Introduction to Head-Related Transfer Functions (H.pdf:/Users/miccio/Zotero/storage/QJD46W73/Cheng - 2001 - Introduction to Head-Related Transfer Functions (H.pdf:application/pdf}
}

@article{spagnol_stima_2016,
	title = {{STIMA} {DI} {FEATURE} {SPETTRALI} {DI} {HRTF} {MEDIANTE} {MODELLI} {ANTROPOMETRICI} {NON} {LINEARI} {PER} {LA} {RESA} {DI} {AUDIO} 3D},
	language = {it},
	author = {Spagnol, Simone and Galesso, Silvio and Avanzini, Federico},
	year = {2016},
	pages = {7},
	file = {Spagnol et al. - 2016 - STIMA DI FEATURE SPETTRALI DI HRTF MEDIANTE MODELL.pdf:/Users/miccio/Zotero/storage/D3KCL88E/Spagnol et al. - 2016 - STIMA DI FEATURE SPETTRALI DI HRTF MEDIANTE MODELL.pdf:application/pdf}
}

@article{baumgartner_modeling_2014,
	title = {Modeling sound-source localization in sagittal planes for human listeners},
	volume = {136},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4887447},
	doi = {10.1121/1.4887447},
	language = {en},
	number = {2},
	urldate = {2019-08-26},
	journal = {The Journal of the Acoustical Society of America},
	author = {Baumgartner, Robert and Majdak, Piotr and Laback, Bernhard},
	month = aug,
	year = {2014},
	pages = {791--802},
	file = {Baumgartner et al. - 2014 - Modeling sound-source localization in sagittal pla.pdf:/Users/miccio/Zotero/storage/VRLEPX9X/Baumgartner et al. - 2014 - Modeling sound-source localization in sagittal pla.pdf:application/pdf}
}

@article{rebillat_estimation_2014,
	title = {Estimation of the low-frequency components of the head-related transfer functions of animals from photographs.},
	volume = {135},
	doi = {10.1121/1.4869087},
	abstract = {Reliable animal head-related transfer function (HRTF) estimation procedures are needed for several practical applications, for example, to investigate the neuronal mechanisms of sound localization using virtual acoustic spaces or to have a quantitative description of the different localization cues available to a given animal species. Here, two established techniques are combined to estimate an animal's HRTF from photographs by taking into account as much morphological detail as possible. The first step of the method consists in building a three-dimensional-model of the animal from pictures taken with a standard camera. The HRTFs are then estimated by means of a rapid boundary-element-method implementation. This combined method is validated on a taxidermist model of a cat by comparing binaural and monaural localization cues extracted from estimated and measured HRTFs. It is shown that it provides a reliable way to estimate low-frequency HRTF, which is difficult to obtain with standard acoustical measurements procedures because of reflections.},
	number = {5},
	journal = {The Journal of the Acoustical Society of America},
	author = {Rébillat, Marc and Benichoux, Victor and Otani, Makoto and Keriven, Renaud and Brette, Romain},
	year = {2014},
	keywords = {Extraction, photograph},
	pages = {2534--2544},
	file = {Full Text PDF:/Users/miccio/Zotero/storage/FK88KCXD/Rébillat et al. - 2014 - Estimation of the low-frequency components of the .pdf:application/pdf}
}

@article{dellepiane_reconstructing_2008,
	title = {Reconstructing head models from photographs for individualized 3D-audio processing},
	volume = {27},
	doi = {10.1111/j.1467-8659.2008.01316.x},
	abstract = {Visual fidelity and interactivity are the main goals in Computer Graphics research, but recently also audio is assuming an important role. Binaural rendering can provide extremely pleasing and realistic three-dimensional sound, but to achieve best results it's necessary either to measure or to estimate individual Head Related Transfer Function (HRTF). This function is strictly related to the peculiar features of ears and face of the listener. Recent sound scattering simulation techniques can calculate HRTF starting from an accurate 3D model of a human head. Hence, the use of binaural rendering on large scale (i.e. video games, entertainment) could depend on the possibility to produce a sufficiently accurate 3D model of a human head, starting from the smallest possible input. In this paper we present a completely automatic system, which produces a 3D model of a head starting from simple input data (five photos and some key-points indicated by user). The geometry is generated by extracting information from images and accordingly deforming a 3D dummy to reproduce user head features. The system proves to be fast, automatic, robust and reliable: geometric validation and preliminary assessments show that it can be accurate enough for HRTF calculation.},
	journal = {Comput. Graph. Forum},
	author = {Dellepiane, Matteo and Pietroni, Nico and Tsingos, Nicolas and Asselot, M. and Scopigno, Roberto},
	year = {2008},
	keywords = {Binaural beats, Simulation, 3D modeling, Computation, Computer graphics, Dummy variable (statistics), Head-related transfer function, Head-up display, Image processing, Interactivity, Morphing, Real-time transcription, Time complexity},
	pages = {1719--1727},
	file = {Submitted Version:/Users/miccio/Zotero/storage/RHS7TL6E/Dellepiane et al. - 2008 - Reconstructing head models from photographs for in.pdf:application/pdf}
}

@article{xie_recovery_2012,
	title = {Recovery of individual head-related transfer functions from a small set of measurements},
	volume = {132},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4728168},
	doi = {10.1121/1.4728168},
	language = {en},
	number = {1},
	urldate = {2019-09-13},
	journal = {The Journal of the Acoustical Society of America},
	author = {Xie, Bo-Sun},
	month = jul,
	year = {2012},
	pages = {282--294}
}

@inproceedings{bharitkar_stacked_2018,
	address = {Honolulu, HI, USA},
	title = {Stacked {Autoencoder} {Based} {HRTF} {Synthesis} from {Sparse} {Data}},
	isbn = {978-988-14768-5-2},
	url = {https://ieeexplore.ieee.org/document/8659495/},
	doi = {10.23919/APSIPA.2018.8659495},
	abstract = {Ipsilateral and contralateral head-related transfer functions (HRTF), Hipsi(ω, r, φ, θ) and Hcontra(ω, r, φ, θ), are used for creating the perception of a virtual sound source at an arbitrary distance r and azimuth-elevation tuple ψ = [θ, φ]T relative to the median plane for a given frequency ω. Publicly available databases use a subset of a full-grid of angular directions due to time and complexity to acquire and deconvolve responses. In this paper, we present a subspace-based technique for reconstructing HRTFs at arbitrary directions for the IRCAM-Listen HRTF database, which comprises a sparse set of HRTFs sampled every 15◦ along the azimuth/elevation direction. The presented technique includes ﬁrst augmenting the sparse IRCAM dataset using auditory localization blur, then deriving a set of lowerdimensional compressed representation (using an autoencoder) from the augmented HRTFs. The lower dimensional representations are then trained using a fully-connected neural network (FCNN) for the corresponding directions. The reconstruction of HRTF corresponding to an arbitrary direction ψ is achieved p by applying the compressed output from the FCNN, for an arbitrary direction, to a reconstruction system (viz., a decoder of an autoencoder). The results demonstrate the autoencoder approach provides good quality objective and subjective results.},
	language = {en},
	urldate = {2019-09-16},
	booktitle = {2018 {Asia}-{Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA} {ASC})},
	publisher = {IEEE},
	author = {Bharitkar, Sunil and Mauer, Timothy and Wells, Teresa and Berfanger, David},
	month = nov,
	year = {2018},
	pages = {356--361},
	file = {Bharitkar et al. - 2018 - Stacked Autoencoder Based HRTF Synthesis from Spar.pdf:/Users/miccio/Zotero/storage/3ZRGS3J3/Bharitkar et al. - 2018 - Stacked Autoencoder Based HRTF Synthesis from Spar.pdf:application/pdf}
}

@article{fabian_hutubs_2019,
	title = {The {HUTUBS} head-related transfer function ({HRTF}) database},
	url = {https://depositonce.tu-berlin.de/handle/11303/9429},
	doi = {10.14279/depositonce-8487},
	abstract = {The database contains head-related transfer functions, 3D head meshes, anthropometric features, and headphone transfer functions of 96 subjects.},
	language = {en},
	urldate = {2019-09-16},
	author = {Fabian, Brinkmann and Manoj, Dinakaran and Robert, Pelzer and Jan Joschka, Wohlgemuth and Fabian, Seipel and Daniel, Voss and Peter, Grosche and Stefan, Weinzierl},
	collaborator = {Technische Universität Berlin and Technische Universität Berlin},
	year = {2019},
	keywords = {600 Technik, Medizin, angewandte Wissenschaften, HRTF}
}

@article{romigh_efficient_2015,
	title = {Efficient {Real} {Spherical} {Harmonic} {Representation} of {Head}-{Related} {Transfer} {Functions}},
	volume = {9},
	issn = {1932-4553, 1941-0484},
	url = {http://ieeexplore.ieee.org/document/7096941/},
	doi = {10.1109/JSTSP.2015.2421876},
	abstract = {Several methods have recently been proposed for modeling spatially continuous head-related transfer functions (HRTFs) using techniques based on ﬁnite-order spherical harmonic expansion. These techniques inherently impart some amount of spatial smoothing to the measured HRTFs. However, the effect this spatial smoothing has on the localization accuracy has not been analyzed. Consequently, the relationship between the order of a spherical harmonic representation for HRTFs and the maximum localization ability that can be achieved with that representation remains unknown. The present study investigates the effect that spatial smoothing has on virtual sound source localization by systematically reducing the order of a spherical-harmonic-based HRTF representation. Results of virtual localization tests indicate that accurate localization performance is retained with spherical harmonic representations as low as fourth-order, and several important physical HRTF cues are shown to be present even in a ﬁrst-order representation. These results suggest that listeners do not rely on the ﬁne details in an HRTF's spatial structure and imply that some of the theoretically-derived bounds for HRTF sampling may be exceeding perceptual requirements.},
	language = {en},
	number = {5},
	urldate = {2019-09-16},
	journal = {IEEE Journal of Selected Topics in Signal Processing},
	author = {Romigh, Griffin D. and Brungart, Douglas S. and Stern, Richard M. and Simpson, Brian D.},
	month = aug,
	year = {2015},
	pages = {921--930},
	file = {Romigh et al. - 2015 - Efficient Real Spherical Harmonic Representation o.pdf:/Users/miccio/Zotero/storage/J4RMIK7E/Romigh et al. - 2015 - Efficient Real Spherical Harmonic Representation o.pdf:application/pdf}
}

@misc{noauthor_ami_nodate,
	title = {{AMI} {Ear} {Database}},
	url = {http://ctim.ulpgc.es/research_works/ami_ear_database/#whole},
	urldate = {2019-09-27},
	file = {AMI Ear Database:/Users/miccio/Zotero/storage/XCYHDTNR/ami_ear_database.html:text/html}
}

@misc{gyawali_disentanglement_2018,
	title = {Disentanglement with {Variational} {Autoencoder}: {A} {Review}},
	shorttitle = {Disentanglement with {Variational} {Autoencoder}},
	url = {https://towardsdatascience.com/disentanglement-with-variational-autoencoder-a-review-653a891b69bd},
	abstract = {Learning of interpretable factorized representation has been around in machine learning for quite a time. But with the recent advancement…},
	language = {en},
	urldate = {2019-10-02},
	journal = {Medium},
	author = {Gyawali, Prashnna K.},
	month = nov,
	year = {2018},
	file = {Snapshot:/Users/miccio/Zotero/storage/U65WDBNV/disentanglement-with-variational-autoencoder-a-review-653a891b69bd.html:text/html}
}

@inproceedings{thuillier_spatial_2018,
	address = {Calgary, AB},
	title = {Spatial {Audio} {Feature} {Discovery} with {Convolutional} {Neural} {Networks}},
	isbn = {978-1-5386-4658-8},
	url = {https://ieeexplore.ieee.org/document/8462315/},
	doi = {10.1109/ICASSP.2018.8462315},
	abstract = {The advent of mixed reality consumer products brings about a pressing need to develop and improve spatial sound rendering techniques for a broad user base. Despite a large body of prior work, the precise nature and importance of various sound localization cues and how they should be personalized for an individual user to improve localization performance is still an open research problem. Here we propose training a convolutional neural network (CNN) to classify the elevation angle of spatially rendered sounds and employing Layerwise Relevance Propagation (LRP) on the trained CNN model. LRP provides saliency maps that can be used to identify spectral features used by the network for classiﬁcation. These maps, in addition to the convolution ﬁlters learned by the CNN, are discussed in the context of listening tests reported in the literature. The proposed approach could potentially provide an avenue for future studies on modeling and personalization of head-related transfer functions (HRTFs).},
	language = {en},
	urldate = {2019-09-19},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Thuillier, Etienne and Gamper, Hannes and Tashev, Ivan J.},
	month = apr,
	year = {2018},
	pages = {6797--6801},
	file = {Thuillier et al. - 2018 - Spatial Audio Feature Discovery with Convolutional.pdf:/Users/miccio/Zotero/storage/VI8RR73D/Thuillier et al. - 2018 - Spatial Audio Feature Discovery with Convolutional.pdf:application/pdf}
}

@article{kistler_model_1992,
	title = {A model of head‐related transfer functions based on principal components analysis and minimum‐phase reconstruction},
	volume = {91},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.402444},
	doi = {10.1121/1.402444},
	language = {en},
	number = {3},
	urldate = {2019-10-02},
	journal = {The Journal of the Acoustical Society of America},
	author = {Kistler, Doris J. and Wightman, Frederic L.},
	month = mar,
	year = {1992},
	pages = {1637--1647},
	file = {Snapshot:/Users/miccio/Zotero/storage/RX2Q3EXQ/pub.html:text/html}
}

@article{zhang_insights_2010,
	title = {Insights into head-related transfer function: {Spatial} dimensionality and continuous representation},
	volume = {127},
	issn = {0001-4966},
	shorttitle = {Insights into head-related transfer function},
	url = {http://asa.scitation.org/doi/10.1121/1.3336399},
	doi = {10.1121/1.3336399},
	language = {en},
	number = {4},
	urldate = {2019-10-02},
	journal = {The Journal of the Acoustical Society of America},
	author = {Zhang, Wen and Abhayapala, Thushara D. and Kennedy, Rodney A. and Duraiswami, Ramani},
	month = apr,
	year = {2010},
	pages = {2347--2357},
	file = {Snapshot:/Users/miccio/Zotero/storage/NJBW5ITB/pub.html:text/html}
}

@article{iida_personalization_2014,
	title = {Personalization of head-related transfer functions in the median plane based on the anthropometry of the listener's pinnae},
	volume = {136},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4880856},
	doi = {10.1121/1.4880856},
	language = {en},
	number = {1},
	urldate = {2019-10-02},
	journal = {The Journal of the Acoustical Society of America},
	author = {Iida, Kazuhiro and Ishii, Yohji and Nishioka, Shinsuke},
	month = jul,
	year = {2014},
	pages = {317--333},
	file = {Snapshot:/Users/miccio/Zotero/storage/4JFKFJ8Z/pub.html:text/html}
}

@article{moller_binaural_1996,
	title = {Binaural {Technique}: {Do} {We} {Need} {Individual} {Recordings}?},
	volume = {44},
	url = {http://www.aes.org/e-lib/browse.cfm?elib=7897},
	number = {6},
	journal = {J. Audio Eng. Soc},
	author = {Møller, Henrik and Sørensen, Michael Friis and Jensen, Clemen Boje and Hammershøi, Dorte},
	year = {1996},
	pages = {451--469}
}

@inproceedings{algazi_cipic_2001,
	address = {New Platz, NY, USA},
	title = {The {CIPIC} {HRTF} database},
	isbn = {978-0-7803-7126-2},
	url = {http://ieeexplore.ieee.org/document/969552/},
	doi = {10.1109/ASPAA.2001.969552},
	urldate = {2019-10-02},
	booktitle = {Proceedings of the 2001 {IEEE} {Workshop} on the {Applications} of {Signal} {Processing} to {Audio} and {Acoustics} ({Cat}. {No}.01TH8575)},
	publisher = {IEEE},
	author = {Algazi, V.R. and Duda, R.O. and Thompson, D.M. and Avendano, C.},
	year = {2001},
	pages = {99--102}
}

@article{muller_transfer-function_2001,
	title = {Transfer-{Function} {Measurement} with {Sweeps}},
	volume = {49},
	url = {http://www.aes.org/e-lib/browse.cfm?elib=10189},
	number = {6},
	journal = {J. Audio Eng. Soc},
	author = {Müller, Swen and Massarani, Paulo},
	year = {2001},
	pages = {443--471},
	file = {Müller and Massarani - 2001 - Transfer-Function Measurement with Sweeps.pdf:/Users/miccio/Zotero/storage/7L6V7Z8M/Müller and Massarani - 2001 - Transfer-Function Measurement with Sweeps.pdf:application/pdf}
}

@inproceedings{guezenoc_hrtf_2018,
	title = {{HRTF} {Individualization}: {A} {Survey}},
	url = {http://www.aes.org/e-lib/browse.cfm?elib=19855},
	booktitle = {Audio {Engineering} {Society} {Convention} 145},
	author = {Guezenoc, Corentin and Seguier, Renaud},
	month = oct,
	year = {2018},
	file = {Guezenoc and Seguier - 2018 - HRTF Individualization A Survey.pdf:/Users/miccio/Zotero/storage/5BV9WHNH/Guezenoc and Seguier - 2018 - HRTF Individualization A Survey.pdf:application/pdf}
}

@inproceedings{kaneko_deepearnet:_2016,
	title = {{DeepEarNet}: {Individualizing} {Spatial} {Audio} with {Photography}, {Ear} {Shape} {Modeling}, and {Neural} {Networks}},
	url = {http://www.aes.org/e-lib/browse.cfm?elib=18509},
	booktitle = {Audio {Engineering} {Society} {Conference}: 2016 {AES} {International} {Conference} on {Audio} for {Virtual} and {Augmented} {Reality}},
	author = {Kaneko, Shoken and Suenaga, Tsukasa and Sekine, Satoshi},
	month = sep,
	year = {2016}
}

@article{yao_head-related_2017,
	title = {Head-{Related} {Transfer} {Function} {Selection} {Using} {Neural} {Networks}},
	volume = {42},
	issn = {2300-262X},
	url = {http://journals.pan.pl/dlibra/publication/117920/edition/102550/content},
	doi = {10.1515/aoa-2017-0038},
	abstract = {Abstract
            In binaural audio systems, for an optimal virtual acoustic space a set of head-related transfer functions (HRTFs) should be used that closely matches the listener’s ones. This study aims to select the most appropriate HRTF dataset from a large database for users without the need for extensive listening tests. Currently, there is no way to reliably reduce the number of datasets to a smaller, more manageable number without risking discarding potentially good matches. A neural network that estimates the appropriateness of HRTF datasets based on input vectors of anthropometric measurements is proposed. The shapes and sizes of listeners’ heads and pinnas were measured using digital photography; the measured anthropometric parameters form the feature vectors used by the neural network. A graphical user interface (GUI) was developed for participants to listen to music transformed using different HRTFs and to evaluate the fitness of each HRTF dataset. The listening scores recorded were the target outputs used to train the neural networks. The aim was to learn a mapping between anthropometric parameters and listener’s perception scores. Experimental validations were performed on 30 subjects. It is demonstrated that the proposed system produces a much more reliable HRTF selection than previously used methods.},
	number = {3},
	urldate = {2019-10-02},
	journal = {Archives of Acoustics},
	author = {Yao, Shu-Nung and Collins, Tim and Liang, Chaoyun},
	month = sep,
	year = {2017},
	pages = {365--373},
	file = {Accepted Version:/Users/miccio/Zotero/storage/82V9SPSY/Yao et al. - 2017 - Head-Related Transfer Function Selection Using Neu.pdf:application/pdf}
}

@article{moller_head-related_1995,
	title = {Head-{Related} {Transfer} {Functions} of {Human} {Subjects}},
	volume = {43},
	url = {http://www.aes.org/e-lib/online/browse.cfm?elib=7949},
	abstract = {Head-related transfer functions (HRTFs) were measured on 40 human subjects for 7 directions of sound incidence, covering the entire sphere. Individual HRTF data for the median, horizontal, and frontal planes are presented in the frequency domain. Measurements were made synchronously at both ears, thus making the time representations, that is, the head-related impulse responses (HRIRs), valid also when interaural time differences are considered. The measurements were made at the entrance to the...},
	language = {English},
	number = {5},
	urldate = {2019-10-02},
	journal = {Journal of the Audio Engineering Society},
	author = {Møller, Henrik and Sørensen, Michael Friis and Hammershøi, Dorte and Jensen, Clemen Boje},
	month = may,
	year = {1995},
	pages = {300--321},
	file = {Møller et al. - 1995 - Head-Related Transfer Functions of Human Subjects.pdf:/Users/miccio/Zotero/storage/A7HCNVEB/Møller et al. - 1995 - Head-Related Transfer Functions of Human Subjects.pdf:application/pdf;Snapshot:/Users/miccio/Zotero/storage/BAKPV4HC/browse.html:text/html}
}

@inproceedings{mokhtari_comparison_2007,
	title = {Comparison of {Simulated} and {Measured} {HRTFs}: {FDTD} {Simulation} {Using} {MRI} {Head} {Data}},
	url = {http://www.aes.org/e-lib/browse.cfm?elib=14298},
	booktitle = {Audio {Engineering} {Society} {Convention} 123},
	author = {Mokhtari, Parham and Takemoto, Hironori and Nishimura, Ryouichi and Kato, Hiroaki},
	month = oct,
	year = {2007}
}

@inproceedings{gumerov_fast_2007,
	title = {Fast {Multipole} {Accelerated} {Boundary} {Elements} for {Numerical} {Computation} of the {Head} {Related} {Transfer} {Function}},
	volume = {1},
	doi = {10.1109/ICASSP.2007.366642},
	abstract = {The numerical computation of head related transfer functions has been attempted by a number of researchers. However, the cost of the computations has meant that usually only low frequencies can be computed and further the computations take inordinately long times. Because of this, comparisons of the computations with measurements are also difficult. We present a fast multipole based iterative preconditioned Krylov solution of a boundary element formulation of the problem and use a new formulation that enables the reciprocity technique to be accurately employed. This allows the calculation to proceed for higher frequencies and larger discretizations. Preliminary results of the computations and of comparisons with measured HRTFs are presented.},
	booktitle = {2007 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} - {ICASSP} '07},
	author = {Gumerov, N. A. and Duraiswami, R. and Zotkin, D. N.},
	month = apr,
	year = {2007},
	keywords = {Acceleration, Acoustic measurements, Acoustic scattering, audio signal processing, boundary element formulation, Boundary element method, Boundary element methods, boundary-elements methods, Costs, Ear, Fast Multipole Method, Frequency, head related transfer function, Head related transfer function, Humans, Irrigation, iterative methods, multipole accelerated boundary elements, multipole based iterative preconditioned Krylov solution, numerical computation, Reciprocity, transfer functions, Transfer functions},
	pages = {I--165--I--168},
	file = {IEEE Xplore Full Text PDF:/Users/miccio/Zotero/storage/2T3GF4CC/Gumerov et al. - 2007 - Fast Multipole Accelerated Boundary Elements for N.pdf:application/pdf}
}

@inproceedings{chen_autoencoding_2019,
	address = {Brighton, United Kingdom},
	title = {Autoencoding {HRTFS} for {DNN} {Based} {HRTF} {Personalization} {Using} {Anthropometric} {Features}},
	isbn = {978-1-4799-8131-1},
	url = {https://ieeexplore.ieee.org/document/8683814/},
	doi = {10.1109/ICASSP.2019.8683814},
	urldate = {2019-10-25},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Chen, Tzu-Yu and Kuo, Tzu-Hsuan and Chi, Tai-Shih},
	month = may,
	year = {2019},
	pages = {271--275},
	file = {Chen et al. - 2019 - Autoencoding HRTFS for DNN Based HRTF Personalizat.pdf:/Users/miccio/Zotero/storage/GJJ4IN9Y/Chen et al. - 2019 - Autoencoding HRTFS for DNN Based HRTF Personalizat.pdf:application/pdf}
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	annote = {http://www.deeplearningbook.org},
	file = {Goodfellow et al. - 2016 - Deep Learning.pdf:/Users/miccio/Zotero/storage/A29TSAT9/Goodfellow et al. - 2016 - Deep Learning.pdf:application/pdf}
}

@article{kingma_introduction_2019,
	title = {An {Introduction} to {Variational} {Autoencoders}},
	volume = {12},
	issn = {1935-8237, 1935-8245},
	url = {http://arxiv.org/abs/1906.02691},
	doi = {10.1561/2200000056},
	abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
	number = {4},
	urldate = {2019-12-27},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Kingma, Diederik P. and Welling, Max},
	year = {2019},
	note = {arXiv: 1906.02691},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {307--392},
	file = {arXiv Fulltext PDF:/Users/miccio/Zotero/storage/MRTF7XJW/Kingma and Welling - 2019 - An Introduction to Variational Autoencoders.pdf:application/pdf;arXiv.org Snapshot:/Users/miccio/Zotero/storage/PMMU872K/1906.html:text/html}
}

@inproceedings{kulkarni_minimum-phase_1995,
	title = {On the minimum-phase approximation of head-related transfer functions},
	doi = {10.1109/ASPAA.1995.482964},
	abstract = {The head-related transfer function embodies the amplitude and phase transformations accompanying a sound source from a fixed position in space to the eardrum. These transformations, which primarily result from the interaction of the acoustic wave with the complex geometry of the head and pinna, are known to be the principal determinants of source location. It is especially known that the interaural time differences (ITDs) are encoded by the phase spectra of the HRTFs at the two ears. In this study we explore a model of the HRTF which approximates the HRTF as a minimum phase function. The model is tested both theoretically and through psychophysical experiments. Results suggest that a minimum phase representation of the phase spectra, augmented by a position-dependent, frequency-independent ITD, is an adequate description of the HRTF phase. Both the scientific and the practical consequences of this result shall be discussed.},
	booktitle = {Proceedings of 1995 {Workshop} on {Applications} of {Signal} {Processing} to {Audio} and {Accoustics}},
	author = {Kulkarni, A. and Isabelle, S.K. and Colburn, H.S.},
	month = oct,
	year = {1995},
	note = {ISSN: null},
	keywords = {Acoustic measurements, acoustic signal processing, acoustic wave, amplitude transformation, Computer displays, Current measurement, Ear, encoding, Geometry, Head, head complex geometry, head-related transfer functions, hearing, interaural time differences, minimum phase function, minimum phase representation, minimum-phase approximation, phase spectra, phase transformation, pinna, Position measurement, psychology, Psychology, psychophysical experiments, sound source, source location, Testing, transfer functions, Transfer functions},
	pages = {84--87},
	file = {IEEE Xplore Full Text PDF:/Users/miccio/Zotero/storage/S3UFZFNI/Kulkarni et al. - 1995 - On the minimum-phase approximation of head-related.pdf:application/pdf}
}

@article{oord_wavenet:_2016,
	title = {{WaveNet}: {A} {Generative} {Model} for {Raw} {Audio}},
	shorttitle = {{WaveNet}},
	url = {http://arxiv.org/abs/1609.03499},
	abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	urldate = {2019-04-17},
	journal = {arXiv:1609.03499 [cs]},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.03499},
	keywords = {Computer Science - Sound, Computer Science - Machine Learning},
	file = {arXiv\:1609.03499 PDF:/Users/miccio/Zotero/storage/GWLUTHZJ/Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:application/pdf;arXiv.org Snapshot:/Users/miccio/Zotero/storage/4PX3AQDD/1609.html:text/html;arXiv.org Snapshot:/Users/miccio/Zotero/storage/FRY24JAC/1609.html:text/html;Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:/Users/miccio/Zotero/storage/REGCM3XP/Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:application/pdf}
}

@inproceedings{sohn_learning_2015,
	title = {Learning {Structured} {Output} {Representation} using {Deep} {Conditional} {Generative} {Models}},
	abstract = {Supervised deep learning has been successfully applied to many recognition problems. Although it can approximate a complex many-to-one function well when a large amount of training data is provided, it is still challenging to model complex structured output representations that effectively perform probabilistic inference and make diverse predictions. In this work, we develop a deep conditional generative model for structured output prediction using Gaussian latent variables. The model is trained efficiently in the framework of stochastic gradient variational Bayes, and allows for fast prediction using stochastic feed-forward inference. In addition, we provide novel strategies to build robust structured prediction algorithms, such as input noise-injection and multi-scale prediction objective at training. In experiments, we demonstrate the effectiveness of our proposed algorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic structured output predictions using stochastic inference. Furthermore, the proposed training methods are complimentary, which leads to strong pixel-level object segmentation and semantic labeling performance on Caltech-UCSD Birds 200 and the subset of Labeled Faces in the Wild dataset.},
	booktitle = {{NIPS}},
	author = {Sohn, Kihyuk and Lee, Honglak and Yan, Xinchen},
	year = {2015},
	keywords = {Approximation algorithm, Artificial neural network, Calculus of variations, Deep learning, Experiment, Generative model, Latent variable, Modal logic, One-to-many (data model), Pixel, Scalability, Stochastic gradient descent, Stochastic neural network, Structured prediction}
}

@inproceedings{miccini_estimation_2019,
	address = {Stockholm},
	title = {{Estimation} {of} {pinna} {notch} {frequency} {from} {anthropometry}: {an} {improved} {linear} {model} {based} {on} {principal} {component} {analysis} {and} {feature} {selection}},
	abstract = {In this paper, anthropometric data from a database of HeadRelated Transfer Functions (HRTFs) is used to estimate the frequency of the ﬁrst pinna notch in the frontal part of the median plane. Given the presence of high correlations between some of the anthropometric features, as well as repeated values for the same subject observations, we propose the introduction of Principal Component Analysis (PCA) to project the features onto a space where they are more separated. We then construct a regression model employing forward step-wise feature selection to choose the principal components most capable of predicting notch frequencies. Our results show that by using a linear regression model with as few as three principal components, we can predict notch frequencies with a cross-validation mean absolute error of just about 600 Hz.},
	language = {en},
	booktitle = {Combined proceedings of the {Nordic} {Sound} and {Music} {Computing} {Conference} 2019 and the {Interactive} {Sonification} {Workshop} 2019},
	author = {Miccini, Riccardo and Spagnol, Simone},
	month = nov,
	year = {2019},
	pages = {4},
	file = {Miccini and Spagnol - ESTIMATION OF PINNA NOTCH FREQUENCY FROM ANTHROPOM.pdf:/Users/miccio/Zotero/storage/9YBGTAI4/Miccini and Spagnol - ESTIMATION OF PINNA NOTCH FREQUENCY FROM ANTHROPOM.pdf:application/pdf}
}

@incollection{bansal_convolutional_2019,
	address = {Singapore},
	title = {Convolutional {Neural} {Network}-{Based} {Human} {Identification} {Using} {Outer} {Ear} {Images}},
	volume = {817},
	isbn = {9789811315947 9789811315954},
	url = {http://link.springer.com/10.1007/978-981-13-1595-4_56},
	abstract = {This paper presents a deep learning approach for ear localization and recognition. The comparable complexity between human outer ear and face in terms of its uniqueness and permanence has increased interest in the use of ear as a biometric. But similar to face recognition, it poses challenges such as illumination, contrast, rotation, scale, and pose variation. Most of the techniques used for ear biometric authentication are based on traditional image processing techniques or handcrafted ensemble features. Owing to extensive work in the ﬁeld of computer vision using convolutional neural networks (CNNs) and histogram of oriented gradients (HOG), the feasibility of deep neural networks (DNNs) in the ﬁeld of ear biometrics has been explored in this research paper. A framework for ear localization and recognition is proposed that aims to reduce the pipeline for a biometric recognition system. The proposed framework uses HOG with support vector machines (SVMs) for ear localization and CNN for ear recognition. CNNs combine feature extraction and ear recognition tasks into one network with an aim to resolve issues such as variations in illumination, contrast, rotation, scale, and pose. The feasibility of the proposed technique has been evaluated on USTB III database. This work demonstrates 97.9\% average recognition accuracy using CNNs without any image preprocessing, which shows that the proposed approach is promising in the ﬁeld of biometric recognition.},
	language = {en},
	urldate = {2019-12-31},
	booktitle = {Soft {Computing} for {Problem} {Solving}},
	publisher = {Springer Singapore},
	author = {Sinha, Harsh and Manekar, Raunak and Sinha, Yash and Ajmera, Pawan K.},
	editor = {Bansal, Jagdish Chand and Das, Kedar Nath and Nagar, Atulya and Deep, Kusum and Ojha, Akshay Kumar},
	year = {2019},
	doi = {10.1007/978-981-13-1595-4_56},
	pages = {707--719},
	file = {Sinha et al. - 2019 - Convolutional Neural Network-Based Human Identific.pdf:/Users/miccio/Zotero/storage/59LR7228/Sinha et al. - 2019 - Convolutional Neural Network-Based Human Identific.pdf:application/pdf}
}

@article{odena2016deconvolution,
  author = {Odena, Augustus and Dumoulin, Vincent and Olah, Chris},
  title = {Deconvolution and Checkerboard Artifacts},
  journal = {Distill},
  year = {2016},
  url = {http://distill.pub/2016/deconv-checkerboard},
  doi = {10.23915/distill.00003}
}
